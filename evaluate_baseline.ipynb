{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2c891687-b516-4e0a-beb1-c84187b70e03",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/calebkumar/repos/stanford/project/odqa-experiments/odqa-experiments/lib/python3.9/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from bert_score import score\n",
    "import pandas as pd\n",
    "from transformers import AutoModelForSeq2SeqLM, AutoTokenizer\n",
    "import time\n",
    "\n",
    "from overlap_evaluate import read_references, read_annotations, ANNOTATIONS, _print_score\n",
    "\n",
    "from evaluate import *\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c547f39d-18d8-466f-b4d0-1875a1231a8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = \"google/t5-xl-ssm-nq\"\n",
    "# model_name = \"google/t5-large-ssm-nq\"\n",
    "\n",
    "# t5_qa_model = AutoModelForSeq2SeqLM.from_pretrained(\"google/t5-large-ssm-nq\")\n",
    "# t5_tok = AutoTokenizer.from_pretrained(\"google/t5-large-ssm-nq\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "81a79e75-3ec9-404f-8be2-e44cc1a7bd91",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "73803a22-c79e-4984-a777-2196cab5253c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SampleQA(QAModel):\n",
    "    model = AutoModelForSeq2SeqLM.from_pretrained(model_name)\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "\n",
    "    def predict_answer(self, question: str, contexts: list) -> str:\n",
    "        input_ids = self.tokenizer(question, return_tensors=\"pt\").input_ids\n",
    "        gen_output = self.model.generate(input_ids)[0]\n",
    "        prediction = self.tokenizer.decode(gen_output, skip_special_tokens=True)\n",
    "\n",
    "        return prediction\n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "37701d29-40ba-4334-9bb2-cccf5bc00a33",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = SampleQA(dataset=QADataset.nq)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6a5a02b-bcc4-428e-8591-63a504cb229a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e67cd955-e87c-4bdf-8117-03e0f918e3ee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Retrieving 3610 predictions\n",
      "At example: 40 after 108.706 seconds. Expecting to take: 161.7002 more minutes\n",
      "At example: 80 after 212.465 seconds. Expecting to take: 156.2503 more minutes\n",
      "At example: 120 after 319.34 seconds. Expecting to take: 154.7912 more minutes\n",
      "At example: 160 after 427.823 seconds. Expecting to take: 153.7489 more minutes\n",
      "At example: 200 after 527.089 seconds. Expecting to take: 149.7811 more minutes\n",
      "At example: 240 after 634.91 seconds. Expecting to take: 148.5866 more minutes\n",
      "At example: 280 after 780.187 seconds. Expecting to take: 154.6442 more minutes\n",
      "At example: 320 after 1026.825 seconds. Expecting to take: 175.9507 more minutes\n",
      "At example: 360 after 1247.56 seconds. Expecting to take: 187.7116 more minutes\n",
      "At example: 400 after 1469.877 seconds. Expecting to take: 196.596 more minutes\n",
      "At example: 440 after 1723.213 seconds. Expecting to take: 206.9161 more minutes\n",
      "At example: 480 after 1975.257 seconds. Expecting to take: 214.672 more minutes\n",
      "At example: 520 after 2221.055 seconds. Expecting to take: 219.9699 more minutes\n",
      "At example: 560 after 2467.609 seconds. Expecting to take: 223.9943 more minutes\n",
      "At example: 600 after 2642.346 seconds. Expecting to take: 220.9295 more minutes\n",
      "At example: 640 after 2791.837 seconds. Expecting to take: 215.9311 more minutes\n",
      "At example: 680 after 3032.076 seconds. Expecting to take: 217.7447 more minutes\n",
      "At example: 720 after 3292.317 seconds. Expecting to take: 220.2499 more minutes\n",
      "At example: 760 after 3545.003 seconds. Expecting to take: 221.5627 more minutes\n",
      "At example: 800 after 3784.956 seconds. Expecting to take: 221.5776 more minutes\n",
      "At example: 840 after 3980.555 seconds. Expecting to take: 218.7726 more minutes\n",
      "At example: 880 after 4230.498 seconds. Expecting to take: 218.736 more minutes\n",
      "At example: 920 after 4492.183 seconds. Expecting to take: 218.9125 more minutes\n",
      "At example: 960 after 4731.385 seconds. Expecting to take: 217.6766 more minutes\n",
      "At example: 1000 after 4998.762 seconds. Expecting to take: 217.4461 more minutes\n",
      "At example: 1040 after 5201.182 seconds. Expecting to take: 214.2153 more minutes\n",
      "At example: 1080 after 5392.714 seconds. Expecting to take: 210.5489 more minutes\n",
      "At example: 1120 after 5550.471 seconds. Expecting to take: 205.6648 more minutes\n",
      "At example: 1160 after 5795.805 seconds. Expecting to take: 204.019 more minutes\n",
      "At example: 1200 after 6041.516 seconds. Expecting to take: 202.223 more minutes\n",
      "At example: 1240 after 6294.524 seconds. Expecting to take: 200.511 more minutes\n",
      "At example: 1280 after 6489.854 seconds. Expecting to take: 196.8927 more minutes\n",
      "At example: 1320 after 6736.846 seconds. Expecting to take: 194.7901 more minutes\n",
      "At example: 1360 after 6971.86 seconds. Expecting to take: 192.2388 more minutes\n",
      "At example: 1400 after 7206.027 seconds. Expecting to take: 189.5871 more minutes\n",
      "At example: 1440 after 7455.311 seconds. Expecting to take: 187.2457 more minutes\n",
      "At example: 1480 after 7628.751 seconds. Expecting to take: 182.9869 more minutes\n",
      "At example: 1520 after 7774.311 seconds. Expecting to take: 178.1613 more minutes\n",
      "At example: 1560 after 7984.954 seconds. Expecting to take: 174.8841 more minutes\n",
      "At example: 1600 after 8235.159 seconds. Expecting to take: 172.4236 more minutes\n",
      "At example: 1640 after 8490.811 seconds. Expecting to take: 169.9888 more minutes\n",
      "At example: 1680 after 8719.895 seconds. Expecting to take: 166.9583 more minutes\n",
      "At example: 1720 after 8895.531 seconds. Expecting to take: 162.9123 more minutes\n",
      "At example: 1760 after 8999.405 seconds. Expecting to take: 157.66 more minutes\n",
      "At example: 1800 after 9101.455 seconds. Expecting to take: 152.5336 more minutes\n",
      "At example: 1840 after 9205.959 seconds. Expecting to take: 147.5955 more minutes\n",
      "At example: 1880 after 9309.754 seconds. Expecting to take: 142.7826 more minutes\n",
      "At example: 1920 after 9408.391 seconds. Expecting to take: 138.0224 more minutes\n",
      "At example: 1960 after 9546.064 seconds. Expecting to take: 133.9371 more minutes\n",
      "At example: 2000 after 9796.879 seconds. Expecting to take: 131.4415 more minutes\n",
      "At example: 2040 after 9904.277 seconds. Expecting to take: 127.0402 more minutes\n",
      "At example: 2080 after 10004.869 seconds. Expecting to take: 122.6558 more minutes\n",
      "At example: 2120 after 10100.489 seconds. Expecting to take: 118.3155 more minutes\n",
      "At example: 2160 after 10204.023 seconds. Expecting to take: 114.1654 more minutes\n",
      "At example: 2200 after 10303.686 seconds. Expecting to take: 110.0621 more minutes\n",
      "At example: 2240 after 10402.863 seconds. Expecting to take: 106.0411 more minutes\n",
      "At example: 2280 after 10512.22 seconds. Expecting to take: 102.2021 more minutes\n",
      "At example: 2320 after 10614.584 seconds. Expecting to take: 98.3679 more minutes\n",
      "At example: 2360 after 10718.513 seconds. Expecting to take: 94.6196 more minutes\n",
      "At example: 2400 after 10827.967 seconds. Expecting to take: 90.985 more minutes\n",
      "At example: 2440 after 10927.421 seconds. Expecting to take: 87.3298 more minutes\n",
      "At example: 2480 after 11023.992 seconds. Expecting to take: 83.7171 more minutes\n",
      "At example: 2520 after 11125.076 seconds. Expecting to take: 80.2006 more minutes\n",
      "At example: 2560 after 11229.079 seconds. Expecting to take: 76.7613 more minutes\n",
      "At example: 2600 after 11336.708 seconds. Expecting to take: 73.3979 more minutes\n",
      "At example: 2640 after 11437.832 seconds. Expecting to take: 70.0423 more minutes\n",
      "At example: 2680 after 11540.289 seconds. Expecting to take: 66.7442 more minutes\n",
      "At example: 2720 after 11645.499 seconds. Expecting to take: 63.5079 more minutes\n",
      "At example: 2760 after 11749.559 seconds. Expecting to take: 60.3087 more minutes\n",
      "At example: 2800 after 11848.186 seconds. Expecting to take: 57.1252 more minutes\n",
      "At example: 2840 after 11948.842 seconds. Expecting to take: 53.9942 more minutes\n",
      "At example: 2880 after 12059.164 seconds. Expecting to take: 50.9444 more minutes\n",
      "At example: 2920 after 12155.262 seconds. Expecting to take: 47.8718 more minutes\n",
      "At example: 2960 after 12262.918 seconds. Expecting to take: 44.8812 more minutes\n",
      "At example: 3000 after 12369.518 seconds. Expecting to take: 41.9189 more minutes\n",
      "At example: 3040 after 12477.144 seconds. Expecting to take: 38.9911 more minutes\n",
      "At example: 3080 after 12583.174 seconds. Expecting to take: 36.0881 more minutes\n",
      "At example: 3120 after 12690.982 seconds. Expecting to take: 33.2189 more minutes\n",
      "At example: 3160 after 12803.797 seconds. Expecting to take: 30.3888 more minutes\n",
      "At example: 3200 after 12910.359 seconds. Expecting to take: 27.569 more minutes\n",
      "At example: 3240 after 13015.158 seconds. Expecting to take: 24.7716 more minutes\n",
      "At example: 3280 after 13121.935 seconds. Expecting to take: 22.0032 more minutes\n",
      "At example: 3320 after 13222.677 seconds. Expecting to take: 19.2499 more minutes\n",
      "At example: 3360 after 13324.796 seconds. Expecting to take: 16.5238 more minutes\n",
      "At example: 3400 after 13426.873 seconds. Expecting to take: 13.8218 more minutes\n",
      "At example: 3440 after 13531.594 seconds. Expecting to take: 11.1452 more minutes\n",
      "At example: 3480 after 13633.581 seconds. Expecting to take: 8.4883 more minutes\n",
      "At example: 3520 after 13737.272 seconds. Expecting to take: 5.854 more minutes\n",
      "At example: 3560 after 13843.656 seconds. Expecting to take: 3.2406 more minutes\n",
      "At example: 3600 after 13949.194 seconds. Expecting to take: 0.6458 more minutes\n"
     ]
    }
   ],
   "source": [
    "predictions = model.get_predictions()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2425cf2f-16b9-47f4-9fd5-132fba7a5dee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Obtaining scores:\n",
      "\n",
      "Scoring annotation label: total\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at roberta-large were not used when initializing RobertaModel: ['lm_head.dense.bias', 'lm_head.dense.weight', 'lm_head.decoder.weight', 'lm_head.layer_norm.weight', 'lm_head.layer_norm.bias', 'lm_head.bias']\n",
      "- This IS expected if you are initializing RobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Warning: Empty candidate sentence detected; setting raw BERTscores to 0.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scoring annotation label: question_overlap\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at roberta-large were not used when initializing RobertaModel: ['lm_head.dense.bias', 'lm_head.dense.weight', 'lm_head.decoder.weight', 'lm_head.layer_norm.weight', 'lm_head.layer_norm.bias', 'lm_head.bias']\n",
      "- This IS expected if you are initializing RobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scoring annotation label: no_question_overlap\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at roberta-large were not used when initializing RobertaModel: ['lm_head.dense.bias', 'lm_head.dense.weight', 'lm_head.decoder.weight', 'lm_head.layer_norm.weight', 'lm_head.layer_norm.bias', 'lm_head.bias']\n",
      "- This IS expected if you are initializing RobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Warning: Empty candidate sentence detected; setting raw BERTscores to 0.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scoring annotation label: answer_overlap\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at roberta-large were not used when initializing RobertaModel: ['lm_head.dense.bias', 'lm_head.dense.weight', 'lm_head.decoder.weight', 'lm_head.layer_norm.weight', 'lm_head.layer_norm.bias', 'lm_head.bias']\n",
      "- This IS expected if you are initializing RobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Warning: Empty candidate sentence detected; setting raw BERTscores to 0.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scoring annotation label: no_answer_overlap\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at roberta-large were not used when initializing RobertaModel: ['lm_head.dense.bias', 'lm_head.dense.weight', 'lm_head.decoder.weight', 'lm_head.layer_norm.weight', 'lm_head.layer_norm.bias', 'lm_head.bias']\n",
      "- This IS expected if you are initializing RobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scoring annotation label: answer_overlap_only\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at roberta-large were not used when initializing RobertaModel: ['lm_head.dense.bias', 'lm_head.dense.weight', 'lm_head.decoder.weight', 'lm_head.layer_norm.weight', 'lm_head.layer_norm.bias', 'lm_head.bias']\n",
      "- This IS expected if you are initializing RobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Warning: Empty candidate sentence detected; setting raw BERTscores to 0.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------------------\n",
      "Label : total\n",
      "N examples  :  3610\n",
      "Exact Match :  32.96398891966759\n",
      "Bert Score :  0.5399286897506936\n",
      "Meteor Score :  0.32245334106687323\n",
      "--------------------------------------------------\n",
      "Label : question_overlap\n",
      "N examples  :  324\n",
      "Exact Match :  73.14814814814815\n",
      "Bert Score :  0.8137125925925922\n",
      "Meteor Score :  0.6647202956693178\n",
      "--------------------------------------------------\n",
      "Label : no_question_overlap\n",
      "N examples  :  672\n",
      "Exact Match :  11.904761904761905\n",
      "Bert Score :  0.3802626488095234\n",
      "Meteor Score :  0.14279529965206905\n",
      "--------------------------------------------------\n",
      "Label : answer_overlap\n",
      "N examples  :  2297\n",
      "Exact Match :  48.23683064867218\n",
      "Bert Score :  0.6716226295167619\n",
      "Meteor Score :  0.4410258872119221\n",
      "--------------------------------------------------\n",
      "Label : no_answer_overlap\n",
      "N examples  :  1313\n",
      "Exact Match :  6.245239908606245\n",
      "Bert Score :  0.3095395125666415\n",
      "Meteor Score :  0.11501911525181006\n",
      "--------------------------------------------------\n",
      "Label : answer_overlap_only\n",
      "N examples  :  315\n",
      "Exact Match :  19.047619047619047\n",
      "Bert Score :  0.47360231746031745\n",
      "Meteor Score :  0.20076735545653787\n"
     ]
    }
   ],
   "source": [
    "saved_results, label_scores = model.evaluate(get_bert_score=True, get_predictions=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "c5719e26-5382-45f3-b333-0b3264c971be",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'total': {'exact_match': 32.96398891966759,\n",
       "  'f1_score': 39.825664640069085,\n",
       "  'bert_score': 0.5399286897506936,\n",
       "  'meteor_score': 0.32245334106687323,\n",
       "  'n_examples': 3610},\n",
       " 'question_overlap': {'exact_match': 73.14814814814815,\n",
       "  'f1_score': 79.96056241426614,\n",
       "  'bert_score': 0.8137125925925922,\n",
       "  'meteor_score': 0.6647202956693178,\n",
       "  'n_examples': 324},\n",
       " 'no_question_overlap': {'exact_match': 11.904761904761905,\n",
       "  'f1_score': 17.830333522297817,\n",
       "  'bert_score': 0.3802626488095234,\n",
       "  'meteor_score': 0.14279529965206905,\n",
       "  'n_examples': 672},\n",
       " 'answer_overlap': {'exact_match': 48.23683064867218,\n",
       "  'f1_score': 54.225600742797084,\n",
       "  'bert_score': 0.6716226295167619,\n",
       "  'meteor_score': 0.4410258872119221,\n",
       "  'n_examples': 2297},\n",
       " 'no_answer_overlap': {'exact_match': 6.245239908606245,\n",
       "  'f1_score': 14.63400186172464,\n",
       "  'bert_score': 0.3095395125666415,\n",
       "  'meteor_score': 0.11501911525181006,\n",
       "  'n_examples': 1313},\n",
       " 'answer_overlap_only': {'em': 0.1905,\n",
       "  'f1': 0.2509,\n",
       "  'bert_score': 0.4736,\n",
       "  'meteor_score': 0.2008,\n",
       "  'total': 315},\n",
       " 'no_overlap': {'em': 0.056,\n",
       "  'f1': 0.1143,\n",
       "  'bert_score': 0.2979,\n",
       "  'meteor_score': 0.0916,\n",
       "  'total': 357}}"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# saved_results[0:5]\n",
    "label_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "acac2adf-3296-461e-90dc-f6db4f84a1c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(saved_results)\n",
    "\n",
    "df.to_csv(f\"results/nq/t5-xl-ssm-nq_results.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "3baa0e1a-aec5-4e6c-a2be-32977cdacfef",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "with open(\"results/nq/t5-large-ssm-nq_results.json\", \"w\") as outfile:\n",
    "    json.dump(label_scores, outfile)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "a9a201b4-a72e-4486-9171-0cdc26093390",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'total': {'exact_match': 32.96398891966759,\n",
       "  'f1_score': 39.825664640069085,\n",
       "  'bert_score': 0.5399286897506936,\n",
       "  'meteor_score': 0.32245334106687323,\n",
       "  'n_examples': 3610},\n",
       " 'question_overlap': {'exact_match': 73.14814814814815,\n",
       "  'f1_score': 79.96056241426614,\n",
       "  'bert_score': 0.8137125925925922,\n",
       "  'meteor_score': 0.6647202956693178,\n",
       "  'n_examples': 324},\n",
       " 'no_question_overlap': {'exact_match': 11.904761904761905,\n",
       "  'f1_score': 17.830333522297817,\n",
       "  'bert_score': 0.3802626488095234,\n",
       "  'meteor_score': 0.14279529965206905,\n",
       "  'n_examples': 672},\n",
       " 'answer_overlap': {'exact_match': 48.23683064867218,\n",
       "  'f1_score': 54.225600742797084,\n",
       "  'bert_score': 0.6716226295167619,\n",
       "  'meteor_score': 0.4410258872119221,\n",
       "  'n_examples': 2297},\n",
       " 'no_answer_overlap': {'exact_match': 6.245239908606245,\n",
       "  'f1_score': 14.63400186172464,\n",
       "  'bert_score': 0.3095395125666415,\n",
       "  'meteor_score': 0.11501911525181006,\n",
       "  'n_examples': 1313},\n",
       " 'answer_overlap_only': {'em': 0.1905,\n",
       "  'f1': 0.2509,\n",
       "  'bert_score': 0.4736,\n",
       "  'meteor_score': 0.2008,\n",
       "  'total': 315},\n",
       " 'no_overlap': {'em': 0.056,\n",
       "  'f1': 0.1143,\n",
       "  'bert_score': 0.2979,\n",
       "  'meteor_score': 0.0916,\n",
       "  'total': 357}}"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "label_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8bd7b442-5ae3-4497-a37b-faa710a26786",
   "metadata": {},
   "outputs": [],
   "source": [
    "# test = pd.read_csv(f\"results/nq/t5-large-ssm-nq_results.csv\").to_dict('records')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "bebc20d7-d585-4b4b-8525-5c1452eb4735",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>total</th>\n",
       "      <th>question_overlap</th>\n",
       "      <th>no_question_overlap</th>\n",
       "      <th>answer_overlap</th>\n",
       "      <th>no_answer_overlap</th>\n",
       "      <th>answer_overlap_only</th>\n",
       "      <th>no_overlap</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>exact_match</th>\n",
       "      <td>28.891967</td>\n",
       "      <td>70.679012</td>\n",
       "      <td>7.440476</td>\n",
       "      <td>44.188071</td>\n",
       "      <td>2.132521</td>\n",
       "      <td>13.333333</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>f1_score</th>\n",
       "      <td>35.182762</td>\n",
       "      <td>77.191603</td>\n",
       "      <td>13.252315</td>\n",
       "      <td>50.266082</td>\n",
       "      <td>8.795566</td>\n",
       "      <td>19.766944</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>bert_score</th>\n",
       "      <td>0.505155</td>\n",
       "      <td>0.792013</td>\n",
       "      <td>0.347671</td>\n",
       "      <td>0.643180</td>\n",
       "      <td>0.263689</td>\n",
       "      <td>0.463488</td>\n",
       "      <td>0.2455</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>meteor_score</th>\n",
       "      <td>0.287185</td>\n",
       "      <td>0.648098</td>\n",
       "      <td>0.107524</td>\n",
       "      <td>0.410552</td>\n",
       "      <td>0.071364</td>\n",
       "      <td>0.164458</td>\n",
       "      <td>0.0573</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>n_examples</th>\n",
       "      <td>3610.000000</td>\n",
       "      <td>324.000000</td>\n",
       "      <td>672.000000</td>\n",
       "      <td>2297.000000</td>\n",
       "      <td>1313.000000</td>\n",
       "      <td>315.000000</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>em</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0224</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>f1</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0750</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>total</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>357.0000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                    total  question_overlap  no_question_overlap  \\\n",
       "exact_match     28.891967         70.679012             7.440476   \n",
       "f1_score        35.182762         77.191603            13.252315   \n",
       "bert_score       0.505155          0.792013             0.347671   \n",
       "meteor_score     0.287185          0.648098             0.107524   \n",
       "n_examples    3610.000000        324.000000           672.000000   \n",
       "em                    NaN               NaN                  NaN   \n",
       "f1                    NaN               NaN                  NaN   \n",
       "total                 NaN               NaN                  NaN   \n",
       "\n",
       "              answer_overlap  no_answer_overlap  answer_overlap_only  \\\n",
       "exact_match        44.188071           2.132521            13.333333   \n",
       "f1_score           50.266082           8.795566            19.766944   \n",
       "bert_score          0.643180           0.263689             0.463488   \n",
       "meteor_score        0.410552           0.071364             0.164458   \n",
       "n_examples       2297.000000        1313.000000           315.000000   \n",
       "em                       NaN                NaN                  NaN   \n",
       "f1                       NaN                NaN                  NaN   \n",
       "total                    NaN                NaN                  NaN   \n",
       "\n",
       "              no_overlap  \n",
       "exact_match          NaN  \n",
       "f1_score             NaN  \n",
       "bert_score        0.2455  \n",
       "meteor_score      0.0573  \n",
       "n_examples           NaN  \n",
       "em                0.0224  \n",
       "f1                0.0750  \n",
       "total           357.0000  "
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.read_json(\"results/nq/t5-large-ssm-nq_results.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ae8aa7bf-3870-499a-8c88-f4ccbd25b483",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from evaluate import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "3e5597e0-6d22-4c25-8496-d3ba8cc0ed9e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'em': 0.0224,\n",
       " 'f1': 0.075,\n",
       " 'bert_score': 0.2455,\n",
       " 'meteor_score': 0.0573,\n",
       " 'total': 357}"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# model.compute_no_overlap_score(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "186df6de-2ff2-4ad2-8709-3028f0935553",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'em': 0.1333,\n",
       " 'f1': 0.1977,\n",
       " 'bert_score': 0.4635,\n",
       " 'meteor_score': 0.1645,\n",
       " 'total': 315}"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# model.compute_answer_overlap_only_scores(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "998dbf4e-52e2-4ae5-a8e4-9dc5d8422eb7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from transformers import BigBirdForQuestionAnswering\n",
    "\n",
    "# model = BigBirdForQuestionAnswering.from_pretrained(\"google/bigbird-base-trivia-itc\")\n",
    "\n",
    "# question = \"How many days are in a year?\"\n",
    "# context = \"There are 365 days in a year.\"\n",
    "# encoded_input = tokenizer(question, context, return_tensors='pt')\n",
    "# output = model(**encoded_input)\n",
    "\n",
    "# tokenizer.decode(output)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "odqa-experiments",
   "language": "python",
   "name": "odqa-experiments"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
