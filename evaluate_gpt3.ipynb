{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1285d841-1f21-48de-b90d-4c24a3b873d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import openai\n",
    "from constants import OPEN_AI_API_KEY\n",
    "\n",
    "from bert_score import score\n",
    "from transformers import AutoModelForSeq2SeqLM, AutoTokenizer\n",
    "import time\n",
    "\n",
    "from overlap_evaluate import read_references, read_annotations, ANNOTATIONS, _print_score\n",
    "\n",
    "from evaluate import *\n",
    "\n",
    "openai.api_key = OPEN_AI_API_KEY"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ee58dfc4-bdcd-43ee-80c3-d0ac8580bf92",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Note production model is davinci\n",
    "# result = openai.Answer.create(\n",
    "#     search_model=\"ada\", \n",
    "#     model=\"curie\", \n",
    "#     question=\"which puppy is happy?\", \n",
    "#     documents=[\"Puppy C is happy.\"], \n",
    "#     examples_context=\"In 2017, U.S. life expectancy was 78.6 years.\", \n",
    "#     examples=[[\"What is human life expectancy in the United States?\", \"78 years.\"]], \n",
    "#     max_rerank=10,\n",
    "#     max_tokens=5,\n",
    "#     stop=[\"\\n\", \"<|endoftext|>\"]\n",
    "# )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d14a4320-5ffe-4d75-9f16-dc0067532810",
   "metadata": {},
   "outputs": [],
   "source": [
    "# \"\"\"\n",
    "# Things to figure out:\n",
    "\n",
    "# 1) how many documents to supply GPT-3 (prob 5-10)\n",
    "# - I say 10, but then set max_rerank to either 5 or 10. \n",
    "\n",
    "# 2) What examples to provide?\n",
    "# -idk. It's almost impossible to evaluate this without running separate experiments. So let's just leave it as-is. \n",
    "\n",
    "# 3) What are max_rerank and max_tokens?\n",
    "# -max_rerank is another reranking layer. I don't actually want it, but I don't think I can remove it either. \n",
    "# -max_tokens I think 5 is okay after looking over NQ\n",
    "\n",
    "# 4) Get a cost estimate based on some examples\n",
    "\n",
    "# \"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1c140717-a981-4f59-a5ed-27fe44b4d733",
   "metadata": {},
   "outputs": [],
   "source": [
    "sample = \"\"\"\n",
    "Abraham Lincoln (/ˈlɪŋkən/; February 12, 1809 – April 15, 1865) was an American lawyer and statesman who served as the 16th president of the United States from 1861 until his assassination in 1865. Lincoln led the nation through the American Civil War and succeeded in preserving the Union, abolishing slavery, bolstering the federal government, and modernizing the U.S. economy. Lincoln was born into poverty in a log cabin in Kentucky and was raised on the frontier, primarily in Indiana. He was self-educated and became a lawyer, Whig Party leader, Illinois state legislator, and U.S. Congressman from Illinois. In 1849, he returned to his law practice but became vexed by the opening of additional lands to slavery as a result of the Kansas–Nebraska Act of 1854. He reentered politics in 1854, becoming a leader in the new Republican Party, and he reached a national audience in the 1858 Senate campaign debates against Stephen Douglas. Lincoln ran for President in 1860, sweeping the North to gain victory. Pro-slavery elements in the South viewed his success as a threat to slavery, and Southern states began seceding from the Union. To secure its independence, the new Confederate States fired on Fort Sumter, a U.S. fort in South Carolina, and Lincoln called up forces to suppress the rebellion and restore the Union. Lincoln, a moderate Republican, had to navigate a contentious array of factions with friends and opponents from both the Democratic and Republican parties. His allies, the War Democrats and the Radical Republicans, demanded harsh treatment of the Southern Confederates. Anti-war Democrats (called \"Copperheads\") despised Lincoln, and irreconcilable pro-Confederate elements plotted his assassination. He managed the factions by exploiting their mutual enmity, carefully distributing political patronage, and by appealing to the American people. His Gettysburg Address appealed to nationalistic, republican, egalitarian, libertarian, and democratic sentiments. Lincoln supervised the strategy and tactics in the war effort, including the selection of generals, and implemented a naval blockade of the South's trade. He suspended habeas corpus in Maryland, and he averted British intervention by defusing the Trent Affair. He engineered the end to slavery with his Emancipation Proclamation, including his order that the Army and Navy liberate, protect, and recruit former slaves. He also encouraged border states to outlaw slavery, and promoted the Thirteenth Amendment to the United States Constitution, which outlawed slavery across the country. Lincoln managed his own successful re-election campaign. He sought to heal the war-torn nation through reconciliation. On April 14, 1865, just days after the war's end at Appomattox, he was attending a play at Ford's Theatre in Washington, D.C., with his wife Mary when he was fatally shot by Confederate sympathizer John Wilkes Booth. Lincoln is remembered as a martyr and hero of the United States and is often ranked as the greatest president in American history.\n",
    "\"\"\"\n",
    "sample_examples = [[\"Who was the 16th president of the United States?\", \"Abraham Lincoln\"], [\"Through what major war did Abraham Lincoln serve as United States president?\", \"American Civil War\"], [\"What year was Abraham Lincoln killed?\", \"1865\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "5c61e98e-811c-47b2-8581-9b52732a51cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "default_sample = \"In 2017, U.S. life expectancy was 78.6 years.\"\n",
    "\n",
    "default_sample_examples = [[\"What is human life expectancy in the United States?\", \"78 years.\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b80e630a-b4f5-4b7b-83bc-6072f561d771",
   "metadata": {},
   "outputs": [],
   "source": [
    "tracker = []\n",
    "class Gpt3QA(QAModel):\n",
    "    \n",
    "    def predict_answer(self, question: str, contexts: list) -> str:\n",
    "        contexts = contexts[0:5]\n",
    "        contexts = [context['text'] for context in contexts]\n",
    "        \n",
    "        result = openai.Answer.create(\n",
    "            search_model=\"davinci\", \n",
    "            model=\"davinci\", \n",
    "            question=question, \n",
    "            documents=contexts, \n",
    "            examples_context=sample, \n",
    "            examples=sample_examples, \n",
    "            max_rerank=5,\n",
    "            max_tokens=5,\n",
    "            stop=[\"\\n\", \"<|endoftext|>\"]\n",
    "        )\n",
    "        tracker.append({\n",
    "            \"question\": question,\n",
    "            \"result\": result\n",
    "        })\n",
    "\n",
    "        return result[\"answers\"][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "9bf875fc-6245-4220-a5a4-001f02652af9",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Gpt3QA(dataset=QADataset.nq, nrows=730)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "e40d8d14-dac8-4d02-84c6-476868661030",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Retrieving 730 predictions\n",
      "At example: 40 after 31.068 seconds. Expecting to take: 8.9321 more minutes\n",
      "At example: 80 after 60.252 seconds. Expecting to take: 8.1591 more minutes\n",
      "At example: 120 after 90.153 seconds. Expecting to take: 7.638 more minutes\n",
      "At example: 160 after 120.362 seconds. Expecting to take: 7.1465 more minutes\n",
      "At example: 200 after 148.941 seconds. Expecting to take: 6.5782 more minutes\n",
      "At example: 240 after 178.732 seconds. Expecting to take: 6.0819 more minutes\n",
      "At example: 280 after 208.096 seconds. Expecting to take: 5.574 more minutes\n",
      "At example: 320 after 237.606 seconds. Expecting to take: 5.0739 more minutes\n",
      "At example: 360 after 268.3 seconds. Expecting to take: 4.5959 more minutes\n",
      "At example: 400 after 296.328 seconds. Expecting to take: 4.0745 more minutes\n",
      "At example: 440 after 326.5 seconds. Expecting to take: 3.5866 more minutes\n",
      "At example: 480 after 356.189 seconds. Expecting to take: 3.0919 more minutes\n",
      "At example: 520 after 386.679 seconds. Expecting to take: 2.6026 more minutes\n",
      "At example: 560 after 416.503 seconds. Expecting to take: 2.1073 more minutes\n",
      "At example: 600 after 446.204 seconds. Expecting to take: 1.6113 more minutes\n",
      "At example: 640 after 475.793 seconds. Expecting to take: 1.1151 more minutes\n",
      "At example: 680 after 505.683 seconds. Expecting to take: 0.6197 more minutes\n",
      "At example: 720 after 535.685 seconds. Expecting to take: 0.124 more minutes\n"
     ]
    }
   ],
   "source": [
    "predictions = model.get_predictions()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "a05f07c2-cf14-4885-8a1d-a4460b6d2bdd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Obtaining scores:\n",
      "\n",
      "Scoring annotation label: total\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at roberta-large were not used when initializing RobertaModel: ['lm_head.bias', 'lm_head.decoder.weight', 'lm_head.layer_norm.bias', 'lm_head.dense.bias', 'lm_head.dense.weight', 'lm_head.layer_norm.weight']\n",
      "- This IS expected if you are initializing RobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Warning: Empty candidate sentence detected; setting raw BERTscores to 0.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scoring annotation label: question_overlap\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at roberta-large were not used when initializing RobertaModel: ['lm_head.bias', 'lm_head.decoder.weight', 'lm_head.layer_norm.bias', 'lm_head.dense.bias', 'lm_head.dense.weight', 'lm_head.layer_norm.weight']\n",
      "- This IS expected if you are initializing RobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scoring annotation label: no_question_overlap\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at roberta-large were not used when initializing RobertaModel: ['lm_head.bias', 'lm_head.decoder.weight', 'lm_head.layer_norm.bias', 'lm_head.dense.bias', 'lm_head.dense.weight', 'lm_head.layer_norm.weight']\n",
      "- This IS expected if you are initializing RobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scoring annotation label: answer_overlap\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at roberta-large were not used when initializing RobertaModel: ['lm_head.bias', 'lm_head.decoder.weight', 'lm_head.layer_norm.bias', 'lm_head.dense.bias', 'lm_head.dense.weight', 'lm_head.layer_norm.weight']\n",
      "- This IS expected if you are initializing RobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scoring annotation label: no_answer_overlap\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at roberta-large were not used when initializing RobertaModel: ['lm_head.bias', 'lm_head.decoder.weight', 'lm_head.layer_norm.bias', 'lm_head.dense.bias', 'lm_head.dense.weight', 'lm_head.layer_norm.weight']\n",
      "- This IS expected if you are initializing RobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Warning: Empty candidate sentence detected; setting raw BERTscores to 0.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scoring annotation label: answer_overlap_only\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at roberta-large were not used when initializing RobertaModel: ['lm_head.bias', 'lm_head.decoder.weight', 'lm_head.layer_norm.bias', 'lm_head.dense.bias', 'lm_head.dense.weight', 'lm_head.layer_norm.weight']\n",
      "- This IS expected if you are initializing RobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------------------\n",
      "Label : total\n",
      "N examples  :  730\n",
      "Exact Match :  28.08219178082192\n",
      "Bert Score :  0.47711445205479397\n",
      "Meteor Score :  0.3123380094167261\n",
      "--------------------------------------------------\n",
      "Label : question_overlap\n",
      "N examples  :  70\n",
      "Exact Match :  38.57142857142857\n",
      "Bert Score :  0.5941308571428571\n",
      "Meteor Score :  0.42076325298536593\n",
      "--------------------------------------------------\n",
      "Label : no_question_overlap\n",
      "N examples  :  141\n",
      "Exact Match :  21.27659574468085\n",
      "Bert Score :  0.4132408510638299\n",
      "Meteor Score :  0.24252388887366658\n",
      "--------------------------------------------------\n",
      "Label : answer_overlap\n",
      "N examples  :  448\n",
      "Exact Match :  33.92857142857143\n",
      "Bert Score :  0.547909107142857\n",
      "Meteor Score :  0.3545561964060013\n",
      "--------------------------------------------------\n",
      "Label : no_answer_overlap\n",
      "N examples  :  282\n",
      "Exact Match :  18.79432624113475\n",
      "Bert Score :  0.3646463120567372\n",
      "Meteor Score :  0.24526798185929582\n",
      "--------------------------------------------------\n",
      "Label : answer_overlap_only\n",
      "N examples  :  61\n",
      "Exact Match :  29.508196721311474\n",
      "Bert Score :  0.5119409836065574\n",
      "Meteor Score :  0.3095120572994318\n"
     ]
    }
   ],
   "source": [
    "saved_results, label_scores = model.evaluate(get_bert_score=True, get_predictions=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "9fdf7a3e-3e6b-44b9-8390-e98b7a3619d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(saved_results)\n",
    "\n",
    "df.to_csv(f\"results/nq/gpt3-nq_results.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "0fae8334-825a-4f4e-ae54-8e97206e1e93",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "with open(\"results/nq/gpt3-nq_results.json\", \"w\") as outfile:\n",
    "    json.dump(label_scores, outfile)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "925dc130-da26-46b0-ba20-4a6506dde260",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'id': 0,\n",
       " 'answers': ['Wilhelm Conrad Röntgen'],\n",
       " 'prediction': 'Wilhelm Röntgen',\n",
       " 'overlap': ['total', 'answer_overlap'],\n",
       " 'em': False,\n",
       " 'f1': 0.8,\n",
       " 'bert_score': 0.90964,\n",
       " 'meteor_score': 0.3448275862068965}"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "saved_results[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "9eeb0662-a53b-4b16-a67c-675f05c70f7a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.15"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "no_overlap = 0\n",
    "total_em = 0\n",
    "for result in saved_results:\n",
    "    if 'no_answer_overlap' in result['overlap'] and 'no_question_overlap' in result['overlap']:\n",
    "        no_overlap+=1\n",
    "        total_em += result['em']\n",
    "\n",
    "total_em / no_overlap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e43534eb-150e-46ed-a464-5ffee91aa61b",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "\n",
    "Label : total\n",
    "N examples  :  50\n",
    "Exact Match :  24.0\n",
    "Bert Score :  0.4017627999999999\n",
    "Meteor Score :  0.3061177000738002\n",
    "--------------------------------------------------\n",
    "Label : question_overlap\n",
    "N examples  :  3\n",
    "Exact Match :  33.333333333333336\n",
    "Bert Score :  0.70457\n",
    "Meteor Score :  0.5842391304347826\n",
    "--------------------------------------------------\n",
    "Label : no_question_overlap\n",
    "N examples  :  7\n",
    "Exact Match :  14.285714285714286\n",
    "Bert Score :  0.5535957142857144\n",
    "Meteor Score :  0.3299637955747401\n",
    "--------------------------------------------------\n",
    "Label : answer_overlap\n",
    "N examples  :  28\n",
    "Exact Match :  28.571428571428573\n",
    "Bert Score :  0.4477910714285715\n",
    "Meteor Score :  0.35637885762034066\n",
    "--------------------------------------------------\n",
    "Label : no_answer_overlap\n",
    "N examples  :  22\n",
    "Exact Match :  18.181818181818183\n",
    "Bert Score :  0.3431818181818183\n",
    "Meteor Score :  0.24214895410547585\n",
    "--------------------------------------------------\n",
    "Label : answer_overlap_only\n",
    "N examples  :  3\n",
    "Exact Match :  0.0\n",
    "Bert Score :  0.4876133333333333\n",
    "Meteor Score :  0.057471264367816084\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d8c8819-8c4c-4584-85e3-1b54fb889e17",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Label : total\n",
    "N examples  :  50\n",
    "Exact Match :  28.0\n",
    "Bert Score :  0.456212\n",
    "Meteor Score :  0.30379518625545215\n",
    "--------------------------------------------------\n",
    "Label : question_overlap\n",
    "N examples  :  3\n",
    "Exact Match :  66.66666666666667\n",
    "Bert Score :  0.9144366666666667\n",
    "Meteor Score :  0.4791666666666667\n",
    "--------------------------------------------------\n",
    "Label : no_question_overlap\n",
    "N examples  :  7\n",
    "Exact Match :  0.0\n",
    "Bert Score :  0.48986\n",
    "Meteor Score :  0.18280980103640201\n",
    "--------------------------------------------------\n",
    "Label : answer_overlap\n",
    "N examples  :  28\n",
    "Exact Match :  35.714285714285715\n",
    "Bert Score :  0.5525464285714287\n",
    "Meteor Score :  0.3562338867936093\n",
    "--------------------------------------------------\n",
    "Label : no_answer_overlap\n",
    "N examples  :  22\n",
    "Exact Match :  18.181818181818183\n",
    "Bert Score :  0.33360454545454554\n",
    "Meteor Score :  0.2370550219341612\n",
    "--------------------------------------------------\n",
    "Label : answer_overlap_only\n",
    "N examples  :  3\n",
    "Exact Match :  0.0\n",
    "Bert Score :  0.5882266666666666\n",
    "Meteor Score :  0.059523809523809514\n",
    "\"\"\""
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "odqa-experiments",
   "language": "python",
   "name": "odqa-experiments"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
