{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "674f0488-b19f-479f-9606-9fc0c2b50935",
   "metadata": {},
   "outputs": [],
   "source": [
    "from bert_score import score\n",
    "import pandas as pd\n",
    "from transformers import AutoModelForSeq2SeqLM, AutoTokenizer\n",
    "import time\n",
    "\n",
    "from nq_evaluate import get_scores, read_references, read_annotations, ANNOTATIONS, _print_score\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63cbd02f-53c4-4d31-a73e-2bd84c3c47a2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2aec2041-7d9f-49b6-8b9e-32a0bee0f397",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at roberta-large were not used when initializing RobertaModel: ['lm_head.layer_norm.weight', 'lm_head.dense.weight', 'lm_head.layer_norm.bias', 'lm_head.dense.bias', 'lm_head.decoder.weight', 'lm_head.bias']\n",
      "- This IS expected if you are initializing RobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "calculating scores...\n",
      "computing bert embedding.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "93ad791640e643ae8bb4357ab1e8c065",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "computing greedy matching.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9db3c57199964155b4fc4889d4cab0e2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done in 0.34 seconds, 2.97 sentences/sec\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(tensor([0.9465]), tensor([0.9550]), tensor([0.9507]))"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cands=[\"hi my name is caleb\"]\n",
    "\n",
    "refs = [\"yo my name is Caleb\"]\n",
    "\n",
    "score(cands, refs, lang='en', verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f1d7bbd6-bcc8-4f05-af3f-cfd9fd83d8d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "annotations = read_annotations(\"data/nq-annotations.jsonl\")\n",
    "references = read_references(\"data/nq-test.qa.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5554d750-7fc9-42ec-9df3-92cd01402feb",
   "metadata": {},
   "outputs": [],
   "source": [
    "nq_test = pd.read_csv(\"data/nq-test.qa.csv\", sep=\"\\t\", names=[\"question\", \"answers\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5f0111d0-260d-4efe-87b8-9529e6f1f87f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from transformers import AutoModelForSeq2SeqLM, AutoTokenizer\n",
    "\n",
    "t5_qa_model = AutoModelForSeq2SeqLM.from_pretrained(\"google/t5-large-ssm-nq\")\n",
    "t5_tok = AutoTokenizer.from_pretrained(\"google/t5-large-ssm-nq\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d05f6697-06b9-415c-959e-0bbb3e0e4a80",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Peyton Manning'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def predict(question: str):    \n",
    "    input_ids = t5_tok(question, return_tensors=\"pt\").input_ids\n",
    "    gen_output = t5_qa_model.generate(input_ids)[0]\n",
    "    prediction = t5_tok.decode(gen_output, skip_special_tokens=True)\n",
    "        \n",
    "    return prediction\n",
    "\n",
    "predict(\"Which quarterback threw for the most passing yards?\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "229e9ca6-4c26-4351-b742-312587dceda2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "At example: 0 after 0.001 seconds\n",
      "At example: 40 after 34.807 seconds\n",
      "At example: 80 after 67.985 seconds\n",
      "At example: 120 after 100.301 seconds\n",
      "At example: 160 after 132.432 seconds\n",
      "At example: 200 after 163.587 seconds\n",
      "At example: 240 after 196.143 seconds\n",
      "At example: 280 after 227.5 seconds\n",
      "At example: 320 after 260.141 seconds\n",
      "At example: 360 after 293.73 seconds\n",
      "At example: 400 after 325.935 seconds\n",
      "At example: 440 after 359.627 seconds\n",
      "At example: 480 after 393.401 seconds\n",
      "At example: 520 after 427.895 seconds\n",
      "At example: 560 after 460.389 seconds\n",
      "At example: 600 after 494.839 seconds\n",
      "At example: 640 after 529.656 seconds\n",
      "At example: 680 after 563.288 seconds\n",
      "At example: 720 after 595.882 seconds\n",
      "At example: 760 after 630.373 seconds\n",
      "At example: 800 after 662.576 seconds\n",
      "At example: 840 after 693.483 seconds\n",
      "At example: 880 after 727.531 seconds\n",
      "At example: 920 after 761.835 seconds\n",
      "At example: 960 after 795.326 seconds\n",
      "At example: 1000 after 831.162 seconds\n",
      "At example: 1040 after 865.783 seconds\n",
      "At example: 1080 after 899.788 seconds\n",
      "At example: 1120 after 934.316 seconds\n",
      "At example: 1160 after 967.785 seconds\n",
      "At example: 1200 after 1000.825 seconds\n",
      "At example: 1240 after 1034.618 seconds\n",
      "At example: 1280 after 1067.207 seconds\n",
      "At example: 1320 after 1099.994 seconds\n",
      "At example: 1360 after 1133.456 seconds\n",
      "At example: 1400 after 1167.2 seconds\n",
      "At example: 1440 after 1201.204 seconds\n",
      "At example: 1480 after 1236.014 seconds\n",
      "At example: 1520 after 1268.917 seconds\n",
      "At example: 1560 after 1303.483 seconds\n",
      "At example: 1600 after 1335.793 seconds\n",
      "At example: 1640 after 1369.519 seconds\n",
      "At example: 1680 after 1403.162 seconds\n",
      "At example: 1720 after 1436.935 seconds\n",
      "At example: 1760 after 1472.893 seconds\n",
      "At example: 1800 after 1506.486 seconds\n",
      "At example: 1840 after 1562.631 seconds\n",
      "At example: 1880 after 1828.154 seconds\n",
      "At example: 1920 after 1873.764 seconds\n",
      "At example: 1960 after 2858.622 seconds\n",
      "At example: 2000 after 3895.546 seconds\n",
      "At example: 2040 after 3956.91 seconds\n",
      "At example: 2080 after 4020.989 seconds\n",
      "At example: 2120 after 4084.584 seconds\n",
      "At example: 2160 after 4151.904 seconds\n",
      "At example: 2200 after 4212.732 seconds\n",
      "At example: 2240 after 4278.422 seconds\n",
      "At example: 2280 after 4342.537 seconds\n",
      "At example: 2320 after 4412.5 seconds\n",
      "At example: 2360 after 4477.517 seconds\n",
      "At example: 2400 after 4547.903 seconds\n",
      "At example: 2440 after 4609.337 seconds\n",
      "At example: 2480 after 4671.797 seconds\n",
      "At example: 2520 after 4736.983 seconds\n",
      "At example: 2560 after 4802.414 seconds\n",
      "At example: 2600 after 4871.873 seconds\n",
      "At example: 2640 after 4929.25 seconds\n",
      "At example: 2680 after 4995.422 seconds\n",
      "At example: 2720 after 5062.716 seconds\n",
      "At example: 2760 after 5130.854 seconds\n",
      "At example: 2800 after 5196.451 seconds\n",
      "At example: 2840 after 5257.898 seconds\n",
      "At example: 2880 after 5323.101 seconds\n",
      "At example: 2920 after 5384.082 seconds\n",
      "At example: 2960 after 5451.112 seconds\n",
      "At example: 3000 after 5517.069 seconds\n",
      "At example: 3040 after 5584.923 seconds\n",
      "At example: 3080 after 5653.069 seconds\n",
      "At example: 3120 after 5718.878 seconds\n",
      "At example: 3160 after 5789.766 seconds\n",
      "At example: 3200 after 5857.238 seconds\n",
      "At example: 3240 after 5924.512 seconds\n",
      "At example: 3280 after 5992.555 seconds\n",
      "At example: 3320 after 6056.567 seconds\n",
      "At example: 3360 after 6121.912 seconds\n",
      "At example: 3400 after 6171.122 seconds\n",
      "At example: 3440 after 6204.782 seconds\n",
      "At example: 3480 after 6237.838 seconds\n",
      "At example: 3520 after 6269.74 seconds\n",
      "At example: 3560 after 6301.682 seconds\n",
      "At example: 3600 after 6335.98 seconds\n",
      "Finished after 6344.192 seconds\n"
     ]
    }
   ],
   "source": [
    "predictions = []\n",
    "start = time.time()\n",
    "\n",
    "for i in range(len(nq_test)):\n",
    "    \n",
    "    if i % 40 == 0:\n",
    "        print(f\"At example: {i} after {round(time.time()-start, 3)} seconds\")\n",
    "    \n",
    "    example = nq_test.iloc[i]\n",
    "    prediction = predict(example['question'])\n",
    "    predictions.append({\"id\": i, \"prediction\": prediction})\n",
    "\n",
    "print(f\"Finished after {round(time.time()-start, 3)} seconds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "be322835-e937-4575-9a5d-ccb314723a77",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'id': 0, 'prediction': 'Wilhelm Conrad Röntgen'}"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predictions[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "083915e1-180f-47ec-91f2-1b02cd15a948",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at roberta-large were not used when initializing RobertaModel: ['lm_head.layer_norm.weight', 'lm_head.dense.weight', 'lm_head.layer_norm.bias', 'lm_head.dense.bias', 'lm_head.decoder.weight', 'lm_head.bias']\n",
      "- This IS expected if you are initializing RobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "calculating scores...\n",
      "computing bert embedding.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fa837a9edc3944d889c71c3f069262d0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/123 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "computing greedy matching.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "709a93b9f5a9445dacbe7cc5cf2da0a9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/102 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warning: Empty candidate sentence detected; setting raw BERTscores to 0.\n",
      "Warning: Empty candidate sentence detected; setting raw BERTscores to 0.\n",
      "Warning: Empty candidate sentence detected; setting raw BERTscores to 0.\n",
      "Warning: Empty candidate sentence detected; setting raw BERTscores to 0.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done in 844.73 seconds, 7.66 sentences/sec\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at roberta-large were not used when initializing RobertaModel: ['lm_head.layer_norm.weight', 'lm_head.dense.weight', 'lm_head.layer_norm.bias', 'lm_head.dense.bias', 'lm_head.decoder.weight', 'lm_head.bias']\n",
      "- This IS expected if you are initializing RobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "calculating scores...\n",
      "computing bert embedding.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8f4381d1b2df429c925db35f69cfe9fb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/12 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "computing greedy matching.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b12c6f263caf4b07bac418f66df4daf8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done in 85.96 seconds, 7.08 sentences/sec\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at roberta-large were not used when initializing RobertaModel: ['lm_head.layer_norm.weight', 'lm_head.dense.weight', 'lm_head.layer_norm.bias', 'lm_head.dense.bias', 'lm_head.decoder.weight', 'lm_head.bias']\n",
      "- This IS expected if you are initializing RobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "calculating scores...\n",
      "computing bert embedding.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2e1e7b364e064b56bace4f6191a7ff65",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/27 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "computing greedy matching.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "220124218383498793f7880db20dee5b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/19 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done in 183.56 seconds, 6.31 sentences/sec\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warning: Empty candidate sentence detected; setting raw BERTscores to 0.\n",
      "Warning: Empty candidate sentence detected; setting raw BERTscores to 0.\n",
      "Some weights of the model checkpoint at roberta-large were not used when initializing RobertaModel: ['lm_head.layer_norm.weight', 'lm_head.dense.weight', 'lm_head.layer_norm.bias', 'lm_head.dense.bias', 'lm_head.decoder.weight', 'lm_head.bias']\n",
      "- This IS expected if you are initializing RobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "calculating scores...\n",
      "computing bert embedding.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "faa6e5fcd4f04c788dd75fa1ef5a185d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/74 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "computing greedy matching.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "75558fbca5ae4ec89de4034557cf0a27",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/70 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warning: Empty candidate sentence detected; setting raw BERTscores to 0.\n",
      "Warning: Empty candidate sentence detected; setting raw BERTscores to 0.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done in 483.62 seconds, 9.15 sentences/sec\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at roberta-large were not used when initializing RobertaModel: ['lm_head.layer_norm.weight', 'lm_head.dense.weight', 'lm_head.layer_norm.bias', 'lm_head.dense.bias', 'lm_head.decoder.weight', 'lm_head.bias']\n",
      "- This IS expected if you are initializing RobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "calculating scores...\n",
      "computing bert embedding.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b8cf8a51bca644a1a85f14fa84776b44",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/52 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "computing greedy matching.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "54ff04defdc54ce4b2a0dbbdd3bf84db",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/32 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done in 373.67 seconds, 5.47 sentences/sec\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warning: Empty candidate sentence detected; setting raw BERTscores to 0.\n",
      "Some weights of the model checkpoint at roberta-large were not used when initializing RobertaModel: ['lm_head.layer_norm.weight', 'lm_head.dense.weight', 'lm_head.layer_norm.bias', 'lm_head.dense.bias', 'lm_head.decoder.weight', 'lm_head.bias']\n",
      "- This IS expected if you are initializing RobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "calculating scores...\n",
      "computing bert embedding.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9e6cb2590e1648858bf0bdc22ade807f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/13 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "computing greedy matching.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8ff4b7c5d0ee48b9bbd830a3b8d010ab",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done in 75.31 seconds, 7.87 sentences/sec\n",
      "--------------------------------------------------\n",
      "Label       : total\n",
      "N examples  :  3610\n",
      "Exact Match :  28.89196675900277\n",
      "Bert Score :  tensor(0.5052)\n",
      "--------------------------------------------------\n",
      "Label       : question_overlap\n",
      "N examples  :  324\n",
      "Exact Match :  70.67901234567901\n",
      "Bert Score :  tensor(0.7920)\n",
      "--------------------------------------------------\n",
      "Label       : no_question_overlap\n",
      "N examples  :  672\n",
      "Exact Match :  7.440476190476191\n",
      "Bert Score :  tensor(0.3477)\n",
      "--------------------------------------------------\n",
      "Label       : answer_overlap\n",
      "N examples  :  2297\n",
      "Exact Match :  44.188071397474964\n",
      "Bert Score :  tensor(0.6432)\n",
      "--------------------------------------------------\n",
      "Label       : no_answer_overlap\n",
      "N examples  :  1313\n",
      "Exact Match :  2.1325209444021325\n",
      "Bert Score :  tensor(0.2637)\n",
      "--------------------------------------------------\n",
      "Label       : answer_overlap_only\n",
      "N examples  :  315\n",
      "Exact Match :  13.333333333333334\n",
      "Bert Score :  tensor(0.4635)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warning: Empty candidate sentence detected; setting raw BERTscores to 0.\n"
     ]
    }
   ],
   "source": [
    "scores = get_scores(predictions, references, annotations, get_bert_score=True)\n",
    "for label in ANNOTATIONS:\n",
    "    _print_score(label, scores[label])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72cde89f-5f26-4cc3-bac1-0a497e6e456b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd961cfd-9988-48a6-9741-d5530eb9d738",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "odqa-experiments",
   "language": "python",
   "name": "odqa-experiments"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
