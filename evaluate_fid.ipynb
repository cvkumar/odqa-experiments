{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3308858d-d90a-4aba-b440-edf7c10c3750",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/calebkumar/repos/stanford/project/odqa-experiments/odqa-experiments/lib/python3.9/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from evaluate import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1d66664d-1062-4894-b49f-8f7493d5bd63",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>question</th>\n",
       "      <th>answers</th>\n",
       "      <th>id</th>\n",
       "      <th>generation</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2707</th>\n",
       "      <td>who got the first nobel prize in physics</td>\n",
       "      <td>[Wilhelm Conrad Röntgen]</td>\n",
       "      <td>0</td>\n",
       "      <td>Wilhelm Conrad Röntgen</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>902</th>\n",
       "      <td>when is the next deadpool movie being released</td>\n",
       "      <td>[May 18, 2018]</td>\n",
       "      <td>1</td>\n",
       "      <td>May 18, 2018</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1805</th>\n",
       "      <td>which mode is used for short wave broadcast se...</td>\n",
       "      <td>[Olivia, MFSK]</td>\n",
       "      <td>2</td>\n",
       "      <td>amplitude modulation</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1354</th>\n",
       "      <td>the south west wind blows across nigeria between</td>\n",
       "      <td>[till September]</td>\n",
       "      <td>3</td>\n",
       "      <td>February and June</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2256</th>\n",
       "      <td>what does hp mean in war and order</td>\n",
       "      <td>[hit points or health points]</td>\n",
       "      <td>4</td>\n",
       "      <td>Hewlett-Packard</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               question  \\\n",
       "2707           who got the first nobel prize in physics   \n",
       "902      when is the next deadpool movie being released   \n",
       "1805  which mode is used for short wave broadcast se...   \n",
       "1354   the south west wind blows across nigeria between   \n",
       "2256                 what does hp mean in war and order   \n",
       "\n",
       "                            answers  id              generation  \n",
       "2707       [Wilhelm Conrad Röntgen]   0  Wilhelm Conrad Röntgen  \n",
       "902                  [May 18, 2018]   1            May 18, 2018  \n",
       "1805                 [Olivia, MFSK]   2    amplitude modulation  \n",
       "1354               [till September]   3       February and June  \n",
       "2256  [hit points or health points]   4         Hewlett-Packard  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results = pd.read_json(\"data/NaturalQuestions_FIDKL_generation/nq_fidkl_large.jsonl\", lines=True)\n",
    "results = results.sort_values(\"id\")\n",
    "results.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "72421be1-f04c-43ff-9da2-cbc76acd2a98",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Wilhelm Conrad Röntgen'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results.loc[results['question'] == \"who got the first nobel prize in physics\"].iloc[0]['generation']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97c94769-3b58-4888-9a44-142aa58f0f35",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "85f7e900-1f17-46a5-b205-47e1f9fa7d6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "tracker = []\n",
    "class FiDQA(QAModel):\n",
    "    results = results \n",
    "    i = 0\n",
    "    def predict_answer(self, question: str, contexts: list) -> str:\n",
    "        prediction = self.results.iloc[self.i]['generation']\n",
    "        self.i += 1\n",
    "        return prediction\n",
    "        # try:\n",
    "        #     prediction = self.results.loc[self.results['question'] == question].iloc[0]['generation']\n",
    "        # except Exception as e:\n",
    "        #     print(f\"Failed on |{question}| with exception {repr(e)}\")\n",
    "        # return prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5ca5759c-3582-4bd0-8892-1f0632708c7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = FiDQA()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db62bde2-6974-4d8f-b739-824d9cc23a4d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "31985ea3-5161-4136-8f63-7796dc3be334",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Retrieving 3610 predictions\n",
      "At example: 40 after 0.003 seconds. Expecting to take: 0.0045 more minutes\n",
      "At example: 80 after 0.007 seconds. Expecting to take: 0.0051 more minutes\n",
      "At example: 120 after 0.01 seconds. Expecting to take: 0.0048 more minutes\n",
      "At example: 160 after 0.013 seconds. Expecting to take: 0.0047 more minutes\n",
      "At example: 200 after 0.017 seconds. Expecting to take: 0.0048 more minutes\n",
      "At example: 240 after 0.02 seconds. Expecting to take: 0.0047 more minutes\n",
      "At example: 280 after 0.023 seconds. Expecting to take: 0.0046 more minutes\n",
      "At example: 320 after 0.026 seconds. Expecting to take: 0.0045 more minutes\n",
      "At example: 360 after 0.03 seconds. Expecting to take: 0.0045 more minutes\n",
      "At example: 400 after 0.033 seconds. Expecting to take: 0.0044 more minutes\n",
      "At example: 440 after 0.036 seconds. Expecting to take: 0.0043 more minutes\n",
      "At example: 480 after 0.039 seconds. Expecting to take: 0.0042 more minutes\n",
      "At example: 520 after 0.043 seconds. Expecting to take: 0.0043 more minutes\n",
      "At example: 560 after 0.046 seconds. Expecting to take: 0.0042 more minutes\n",
      "At example: 600 after 0.049 seconds. Expecting to take: 0.0041 more minutes\n",
      "At example: 640 after 0.052 seconds. Expecting to take: 0.004 more minutes\n",
      "At example: 680 after 0.054 seconds. Expecting to take: 0.0039 more minutes\n",
      "At example: 720 after 0.057 seconds. Expecting to take: 0.0038 more minutes\n",
      "At example: 760 after 0.06 seconds. Expecting to take: 0.0038 more minutes\n",
      "At example: 800 after 0.063 seconds. Expecting to take: 0.0037 more minutes\n",
      "At example: 840 after 0.066 seconds. Expecting to take: 0.0036 more minutes\n",
      "At example: 880 after 0.069 seconds. Expecting to take: 0.0036 more minutes\n",
      "At example: 920 after 0.072 seconds. Expecting to take: 0.0035 more minutes\n",
      "At example: 960 after 0.075 seconds. Expecting to take: 0.0035 more minutes\n",
      "At example: 1000 after 0.078 seconds. Expecting to take: 0.0034 more minutes\n",
      "At example: 1040 after 0.081 seconds. Expecting to take: 0.0033 more minutes\n",
      "At example: 1080 after 0.084 seconds. Expecting to take: 0.0033 more minutes\n",
      "At example: 1120 after 0.087 seconds. Expecting to take: 0.0032 more minutes\n",
      "At example: 1160 after 0.09 seconds. Expecting to take: 0.0032 more minutes\n",
      "At example: 1200 after 0.092 seconds. Expecting to take: 0.0031 more minutes\n",
      "At example: 1240 after 0.095 seconds. Expecting to take: 0.003 more minutes\n",
      "At example: 1280 after 0.098 seconds. Expecting to take: 0.003 more minutes\n",
      "At example: 1320 after 0.101 seconds. Expecting to take: 0.0029 more minutes\n",
      "At example: 1360 after 0.104 seconds. Expecting to take: 0.0029 more minutes\n",
      "At example: 1400 after 0.107 seconds. Expecting to take: 0.0028 more minutes\n",
      "At example: 1440 after 0.11 seconds. Expecting to take: 0.0028 more minutes\n",
      "At example: 1480 after 0.113 seconds. Expecting to take: 0.0027 more minutes\n",
      "At example: 1520 after 0.116 seconds. Expecting to take: 0.0027 more minutes\n",
      "At example: 1560 after 0.119 seconds. Expecting to take: 0.0026 more minutes\n",
      "At example: 1600 after 0.122 seconds. Expecting to take: 0.0026 more minutes\n",
      "At example: 1640 after 0.125 seconds. Expecting to take: 0.0025 more minutes\n",
      "At example: 1680 after 0.128 seconds. Expecting to take: 0.0025 more minutes\n",
      "At example: 1720 after 0.131 seconds. Expecting to take: 0.0024 more minutes\n",
      "At example: 1760 after 0.134 seconds. Expecting to take: 0.0023 more minutes\n",
      "At example: 1800 after 0.136 seconds. Expecting to take: 0.0023 more minutes\n",
      "At example: 1840 after 0.139 seconds. Expecting to take: 0.0022 more minutes\n",
      "At example: 1880 after 0.142 seconds. Expecting to take: 0.0022 more minutes\n",
      "At example: 1920 after 0.145 seconds. Expecting to take: 0.0021 more minutes\n",
      "At example: 1960 after 0.148 seconds. Expecting to take: 0.0021 more minutes\n",
      "At example: 2000 after 0.151 seconds. Expecting to take: 0.002 more minutes\n",
      "At example: 2040 after 0.154 seconds. Expecting to take: 0.002 more minutes\n",
      "At example: 2080 after 0.157 seconds. Expecting to take: 0.0019 more minutes\n",
      "At example: 2120 after 0.16 seconds. Expecting to take: 0.0019 more minutes\n",
      "At example: 2160 after 0.163 seconds. Expecting to take: 0.0018 more minutes\n",
      "At example: 2200 after 0.166 seconds. Expecting to take: 0.0018 more minutes\n",
      "At example: 2240 after 0.169 seconds. Expecting to take: 0.0017 more minutes\n",
      "At example: 2280 after 0.172 seconds. Expecting to take: 0.0017 more minutes\n",
      "At example: 2320 after 0.174 seconds. Expecting to take: 0.0016 more minutes\n",
      "At example: 2360 after 0.177 seconds. Expecting to take: 0.0016 more minutes\n",
      "At example: 2400 after 0.18 seconds. Expecting to take: 0.0015 more minutes\n",
      "At example: 2440 after 0.183 seconds. Expecting to take: 0.0015 more minutes\n",
      "At example: 2480 after 0.186 seconds. Expecting to take: 0.0014 more minutes\n",
      "At example: 2520 after 0.189 seconds. Expecting to take: 0.0014 more minutes\n",
      "At example: 2560 after 0.192 seconds. Expecting to take: 0.0013 more minutes\n",
      "At example: 2600 after 0.195 seconds. Expecting to take: 0.0013 more minutes\n",
      "At example: 2640 after 0.198 seconds. Expecting to take: 0.0012 more minutes\n",
      "At example: 2680 after 0.201 seconds. Expecting to take: 0.0012 more minutes\n",
      "At example: 2720 after 0.204 seconds. Expecting to take: 0.0011 more minutes\n",
      "At example: 2760 after 0.207 seconds. Expecting to take: 0.0011 more minutes\n",
      "At example: 2800 after 0.209 seconds. Expecting to take: 0.001 more minutes\n",
      "At example: 2840 after 0.212 seconds. Expecting to take: 0.001 more minutes\n",
      "At example: 2880 after 0.215 seconds. Expecting to take: 0.0009 more minutes\n",
      "At example: 2920 after 0.218 seconds. Expecting to take: 0.0009 more minutes\n",
      "At example: 2960 after 0.221 seconds. Expecting to take: 0.0008 more minutes\n",
      "At example: 3000 after 0.224 seconds. Expecting to take: 0.0008 more minutes\n",
      "At example: 3040 after 0.227 seconds. Expecting to take: 0.0007 more minutes\n",
      "At example: 3080 after 0.23 seconds. Expecting to take: 0.0007 more minutes\n",
      "At example: 3120 after 0.233 seconds. Expecting to take: 0.0006 more minutes\n",
      "At example: 3160 after 0.236 seconds. Expecting to take: 0.0006 more minutes\n",
      "At example: 3200 after 0.239 seconds. Expecting to take: 0.0005 more minutes\n",
      "At example: 3240 after 0.242 seconds. Expecting to take: 0.0005 more minutes\n",
      "At example: 3280 after 0.245 seconds. Expecting to take: 0.0004 more minutes\n",
      "At example: 3320 after 0.248 seconds. Expecting to take: 0.0004 more minutes\n",
      "At example: 3360 after 0.251 seconds. Expecting to take: 0.0003 more minutes\n",
      "At example: 3400 after 0.254 seconds. Expecting to take: 0.0003 more minutes\n",
      "At example: 3440 after 0.257 seconds. Expecting to take: 0.0002 more minutes\n",
      "At example: 3480 after 0.26 seconds. Expecting to take: 0.0002 more minutes\n",
      "At example: 3520 after 0.263 seconds. Expecting to take: 0.0001 more minutes\n",
      "At example: 3560 after 0.266 seconds. Expecting to take: 0.0001 more minutes\n",
      "At example: 3600 after 0.269 seconds. Expecting to take: 0.0 more minutes\n"
     ]
    }
   ],
   "source": [
    "preds = model.get_predictions()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "419e9fd5-50ad-4c87-9471-1d60367e2411",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Obtaining scores:\n",
      "\n",
      "Scoring annotation label: total\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at roberta-large were not used when initializing RobertaModel: ['lm_head.decoder.weight', 'lm_head.bias', 'lm_head.layer_norm.bias', 'lm_head.layer_norm.weight', 'lm_head.dense.weight', 'lm_head.dense.bias']\n",
      "- This IS expected if you are initializing RobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Warning: Empty candidate sentence detected; setting raw BERTscores to 0.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scoring annotation label: question_overlap\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at roberta-large were not used when initializing RobertaModel: ['lm_head.decoder.weight', 'lm_head.bias', 'lm_head.layer_norm.bias', 'lm_head.layer_norm.weight', 'lm_head.dense.weight', 'lm_head.dense.bias']\n",
      "- This IS expected if you are initializing RobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scoring annotation label: no_question_overlap\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at roberta-large were not used when initializing RobertaModel: ['lm_head.decoder.weight', 'lm_head.bias', 'lm_head.layer_norm.bias', 'lm_head.layer_norm.weight', 'lm_head.dense.weight', 'lm_head.dense.bias']\n",
      "- This IS expected if you are initializing RobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Warning: Empty candidate sentence detected; setting raw BERTscores to 0.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scoring annotation label: answer_overlap\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at roberta-large were not used when initializing RobertaModel: ['lm_head.decoder.weight', 'lm_head.bias', 'lm_head.layer_norm.bias', 'lm_head.layer_norm.weight', 'lm_head.dense.weight', 'lm_head.dense.bias']\n",
      "- This IS expected if you are initializing RobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Warning: Empty candidate sentence detected; setting raw BERTscores to 0.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scoring annotation label: no_answer_overlap\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at roberta-large were not used when initializing RobertaModel: ['lm_head.decoder.weight', 'lm_head.bias', 'lm_head.layer_norm.bias', 'lm_head.layer_norm.weight', 'lm_head.dense.weight', 'lm_head.dense.bias']\n",
      "- This IS expected if you are initializing RobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scoring annotation label: answer_overlap_only\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at roberta-large were not used when initializing RobertaModel: ['lm_head.decoder.weight', 'lm_head.bias', 'lm_head.layer_norm.bias', 'lm_head.layer_norm.weight', 'lm_head.dense.weight', 'lm_head.dense.bias']\n",
      "- This IS expected if you are initializing RobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Warning: Empty candidate sentence detected; setting raw BERTscores to 0.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------------------\n",
      "Label : total\n",
      "N examples  :  3610\n",
      "Exact Match :  53.13019390581717\n",
      "Bert Score :  0.6836218864265929\n",
      "Meteor Score :  0.4788400328518001\n",
      "--------------------------------------------------\n",
      "Label : question_overlap\n",
      "N examples  :  324\n",
      "Exact Match :  76.23456790123457\n",
      "Bert Score :  0.8364304629629628\n",
      "Meteor Score :  0.6609091909421637\n",
      "--------------------------------------------------\n",
      "Label : no_question_overlap\n",
      "N examples  :  672\n",
      "Exact Match :  42.11309523809524\n",
      "Bert Score :  0.6087053571428568\n",
      "Meteor Score :  0.3985079594415302\n",
      "--------------------------------------------------\n",
      "Label : answer_overlap\n",
      "N examples  :  2297\n",
      "Exact Match :  63.77884196778407\n",
      "Bert Score :  0.767136530256857\n",
      "Meteor Score :  0.5394593015853919\n",
      "--------------------------------------------------\n",
      "Label : no_answer_overlap\n",
      "N examples  :  1313\n",
      "Exact Match :  34.5011424219345\n",
      "Bert Score :  0.5375189794364055\n",
      "Meteor Score :  0.37279093895913895\n",
      "--------------------------------------------------\n",
      "Label : answer_overlap_only\n",
      "N examples  :  315\n",
      "Exact Match :  47.61904761904762\n",
      "Bert Score :  0.6746063809523815\n",
      "Meteor Score :  0.4173547951997344\n"
     ]
    }
   ],
   "source": [
    "saved_results, scores_per_label = model.evaluate(get_bert_score=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c715595f-f1e3-48ae-a716-e88b9b8bfcac",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "0aee9fa1-ac1b-402c-a52e-627c94cc674c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import json\n",
    "\n",
    "# with open(\"results/nq/fid-large-nq_results.json\", \"w\") as outfile:\n",
    "#     json.dump(model.scores_per_label, outfile)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "4e571880-5131-4f67-8579-3fca2eb7ed28",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(saved_results)\n",
    "\n",
    "df.to_csv(f\"results/nq/fid-large-nq_results.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d1f5ad6-483f-42e9-b3bd-65d40880dadc",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "odqa-experiments",
   "language": "python",
   "name": "odqa-experiments"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
